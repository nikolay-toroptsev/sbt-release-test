hadoop {
  kerberos {
    principal = "sm-data_EXTRA_NEW"
    keytab = "/run/secrets/client.keytab"
  }
}

spark-config {
  spark.eventLog.dir = "hdfs://nameservice1/user/spark/applicationHistory"
  spark.eventLog.enabled = "true"
  spark.serializer = "org.apache.spark.serializer.KryoSerializer"
  spark.shuffle.service.enabled = "true"
  spark.shuffle.service.port = "7337"
  spark.yarn.historyServer.address = "http://prod-jh.offline-analytics.sel-vpc.onefactor.com:18089"
  spark.master = "yarn"
  spark.pyspark.python = "/usr/bin/python3.6"
  spark.sm.admin.uri = "http://prod-vars-admin001.ml.sel-vpc.onefactor.com:19091"
  spark.cleaner.referenceTracking.cleanCheckpoints = "true"
  spark.sql.parquet.compression.codec = "gzip"
  spark.cleaner.periodicGC.interval = "10min"
  spark.onef.dataproviderid="megafon"
  spark.sql.sources.partitionColumnTypeInference.enabled = "false"
  spark.sql.session.timeZone = "UTC"
  spark.driver.extraJavaOptions = "-Duser.timezone=UTC"
  spark.executor.extraJavaOptions = "-Duser.timezone=UTC"
  spark.sql.adaptive.enabled = "true"
  spark.sql.autoBroadcastJoinThreshold = "1048576" # 1M
  spark.sql.broadcastTimeout = "600"
  spark.sql.shuffle.partitions = "2500"
  spark.yarn.jars = "hdfs://nameservice1/user/spark/spark-2.3.2/*"
}

dataset-config {
  spark.onef.contextClass = "com.onefactor.mlengine.datam.context.hdfs.HDFSPartitionBasedContext",

  spark.onef.dataset.geotrack.path = "/onefactor/derivative/geo/stream/geo_track",
  spark.onef.dataset.geotrack.date_partitioned = "true"
  spark.onef.dataset.geotrack.frequency = "daily"
  spark.onef.dataset.geotrack.delay = "1"

  spark.onef.dataset.cell_coverage.path = "/onefactor/masterdata/snapshot/cell_coverage/archive/dp=megafon",
  spark.onef.dataset.cell_coverage.date_partitioned = "true"
  spark.onef.dataset.cell_coverage.frequency = "daily"
  spark.onef.dataset.cell_coverage.delay = "1"

  spark.onef.dataset.charges.path = "/megafon/derivative/stream/rich_charge_report",
  spark.onef.dataset.charges.date_partitioned = "true"
  spark.onef.dataset.charges.frequency = "daily"
  spark.onef.dataset.charges.delay = "1"

  spark.onef.dataset.banks_alphanum.path = "/shared/yury.zhuravlev/banks_alphanum"
  spark.onef.dataset.banks_alphanum.view = "true"

  spark.onef.dataset.poi_home.path = "/onefactor/derivative/geo/snapshot/legacy_home/archive"
  spark.onef.dataset.poi_home.date_partitioned = "true"
  spark.onef.dataset.poi_home.frequency = "weekly"
  spark.onef.dataset.poi_home.delay = "1"

  spark.onef.dataset.poi_work.path = "/onefactor/derivative/geo/snapshot/legacy_work/archive"
  spark.onef.dataset.poi_work.date_partitioned = "true"
  spark.onef.dataset.poi_work.frequency = "weekly"
  spark.onef.dataset.poi_work.delay = "1"

  spark.onef.dataset.region_weights.path = "/onefactor/derivative/geo/snapshot/region_weight/archive"
  spark.onef.dataset.region_weights.date_partitioned = "true"
  spark.onef.dataset.region_weights.frequency = "monthly"
  spark.onef.dataset.region_weights.delay = "34"

  spark.onef.dataset.region_shape.path = "/shared/yury.zhuravlev/region_wkt/actual_parquet"

  spark.onef.dataset.region_wkt.path = "/onefactor/masterdata/dict/region_wkt/actual/country=RU",
  spark.onef.dataset.region_wkt.hdfs_format = "csv: header=true, delimiter=tab",

  spark.onef.dataset.region_tz.path = "/onefactor/masterdata/dict/region/actual"
  spark.onef.dataset.region_tz.hdfs_format = "csv: header=true, delimiter=tab",

  spark.onef.dataset.places_population.path = "/onefactor/derivative/service/gm/datasets/places_population/archive"
  spark.onef.dataset.places_population.date_partitioned = "true"
  spark.onef.dataset.places_population.frequency = "monthly"
  spark.onef.dataset.places_population.delay = "35"

  spark.onef.dataset.osm_poi.path = "/onefactor/masterdata/snapshot/osm_poi/archive"
  spark.onef.dataset.osm_poi.date_partitioned = "true"
  spark.onef.dataset.osm_poi.frequency = "weekly"
  spark.onef.dataset.osm_poi.delay = "1"

  spark.onef.dataset.osm_places.path = "/onefactor/masterdata/snapshot/osm_places/archive"
  spark.onef.dataset.osm_places.date_partitioned = "true"
  spark.onef.dataset.osm_places.frequency = "weekly"
  spark.onef.dataset.osm_places.delay = "1"

  spark.onef.dataset.poi_cell_visits.path = "/onefactor/derivative/service/gm/datasets/poi_cell_visits/archive"
  spark.onef.dataset.poi_cell_visits.date_partitioned = "true"
  spark.onef.dataset.poi_cell_visits.frequency = "daily"
  spark.onef.dataset.poi_cell_visits.delay = "1"

  spark.onef.dataset.msisdn_weights.path = "/onefactor/derivative/geo/snapshot/subscriber_weight_region/archive"
  spark.onef.dataset.msisdn_weights.date_partitioned = "true"
  spark.onef.dataset.msisdn_weights.frequency = "monthly"
  spark.onef.dataset.msisdn_weights.delay = "34"

  spark.onef.dataset.road_graph_cluster.path = "/onefactor/derivative/geo/snapshot/road_simplified_cluster_edge/archive"
  spark.onef.dataset.road_graph_cluster.date_partitioned = "true"
  spark.onef.dataset.road_graph_cluster.frequency = "weekly"
  spark.onef.dataset.road_graph_cluster.delay = "1"

  spark.onef.dataset.road_graph_mapping.path = "/onefactor/derivative/service/gm/datasets/road_graph_mapping/archive"
  spark.onef.dataset.road_graph_mapping.date_partitioned = "true"
  spark.onef.dataset.road_graph_mapping.frequency = "weekly"
  spark.onef.dataset.road_graph_mapping.delay = "2"

  spark.onef.dataset.road_track.path = "/onefactor/derivative/geo/stream/road_track"
  spark.onef.dataset.road_track.date_partitioned = "true"
  spark.onef.dataset.road_track.frequency = "daily"
  spark.onef.dataset.road_track.delay = "1"

  // socdem
  spark.onef.dataset.age.path = "/onefactor/derivative/service/gm/datasets/age"
  spark.onef.dataset.gender.path = "/onefactor/derivative/service/gm/datasets/gender"
  spark.onef.dataset.income.path = "/onefactor/derivative/service/gm/datasets/income"

  spark.onef.dataset.subs_profile.path = "/onefactor/derivative/geo/snapshot/subs_profile_14"
  spark.onef.dataset.subs_profile.date_partitioned = "true"
  spark.onef.dataset.subs_profile.frequency = "monthly"
  spark.onef.dataset.subs_profile.delay = "40"

  spark.onef.dataset.location_tile_msisdn.sampling_factor = "5"
  spark.onef.dataset.location_stay_tile_msisdn.sampling_factor = "5"
  spark.onef.dataset.location_tx_msisdn.sampling_factor = "5"
}

jupyter  {
  spark-config = ${dataset-config} {
    spark.hadoop.dfs.replication = "2"
    spark.dynamicAllocation.enabled = "true"
    spark.dynamicAllocation.executorIdleTimeout = "5m"
    spark.dynamicAllocation.cachedExecutorIdleTimeout = "1h"
    spark.dynamicAllocation.schedulerBacklogTimeout = "10s"
    spark.dynamicAllocation.maxExecutors = "120"
    spark.executor.cores = "2"
    spark.executor.memory = "7746M"
    spark.executor.memoryOverhead = "4G"
    spark.driver.memory = "16G"
    spark.driver.cores = "4"
    spark.driver.maxResultSize = "6G"
    spark.yarn.queue = "root.adhoc"
    spark.sql.autoBroadcastJoinThreshold = "67108864" # 64M
    spark.kryoserializer.buffer.max = "1024M"
    spark.sql.codegen.wholeStage = "false" // disable wholeStage codegen because of wide datasets
    spark.port.maxRetries = "256"
    spark.onef.ml.core.pipeline.broadcast = "true"
    spark.pyspark.driver.python="/usr/bin/python3.6"
    spark.sql.catalogImplementation="in-memory"
    spark.onef.ml.repository.api=${ml-reposiotry-client}
    spark.sql.adaptive.enabled="true"
  }
}

hadoop-config {
  url="https://prod-cm.offline-analytics.sel-vpc.onefactor.com:7183/cmf/services/8/client-config"
}

ml-reposiotry-client {
  target = "repository-service-1-0-2.ml-engine.default.test.sel-vpc.onefactor.com:4099"
  max-inbound-message-size = "16777216"
}

ml-repository {
  db {
    source {
      properties = {
        serverName = "postgres.ml-engine.default.test.sel-vpc.onefactor.com"
        portNumber = 5432
        databaseName = "ml_repository"
        user = "postgres"
        password = "postgres"
      }
    }
  }

  server {
    max-message-size = "16MiB"
  }
}

ml-batch {
  batch-job {
    config = ${dataset-config} {
      spark.dynamicAllocation.enabled = "true"
      spark.dynamicAllocation.executorIdleTimeout = "10m"
      spark.dynamicAllocation.cachedExecutorIdleTimeout = "6h"
      spark.dynamicAllocation.schedulerBacklogTimeout = "10s"
      spark.dynamicAllocation.maxExecutors = "100"
      spark.dynamicAllocation.minExecutors = "10"
      spark.executor.cores = "4"
      spark.executor.memory = "12G"
      spark.executor.memoryOverhead = "4G"
      spark.driver.memory = "16G"
      spark.driver.memoryOverhead = "4G"
      spark.driver.cores = "4"
      spark.driver.maxResultSize = "6G"
      spark.yarn.queue = "sm.batch"
      spark.sql.autoBroadcastJoinThreshold = "1048576" # 1M
      spark.sql.broadcastTimeout = "600"
      spark.kryoserializer.buffer.max="1024M"
      spark.sql.codegen.wholeStage = "false" // disable wholeStage codegen because of wide datasets
      spark.onef.checkpoint.dir = "/tmp/sm-batch/.checkpoints"
      spark.onef.ml.core.pipeline.broadcast = "false"
    }
  }

  storage {
    path = "hdfs://nameservice1/shared/sm-data/ml-batch"
  }

  repository = ${ml-reposiotry-client}

  hadoop = ${hadoop}
}

ml-online {
  models {
    execution {
      checkpoints-path = "file:/tmp/online-service/checkpoints"
    }
  }

  hadoop = ${hadoop}

  spark-config = ${spark-config} {
    spark.driver.cores=10
    spark.testing.memory=2147483648
    spark.onef.ml.repository.api=${ml-reposiotry-client}
    spark.hadoop.hadoop.tmp.dir="/shared/"${hadoop.kerberos.principal}"/.tmp"
  }
}
